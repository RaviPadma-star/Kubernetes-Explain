1. create a new iam user with admin access.. in real time we are not giving admin access save secret access key in local
2. create a workstation called a instance from there we should connect eks cluster means provision of cluster this is for only lab purpose
there is always a prodcution provision cluster
3. eksctl --> means its connect the aws console
4. cmd --> aws --version
 ---aws-cli/1.18.147 Python/2.7.18 Linux/5.10.176-157.645.amzn2.x86_64 botocore/1.18.6
here we getting aws v1 but need is aws 2 version for that search aws cli 2 version

-------------------

curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

Note
To update your current installation of the AWS CLI, add your existing symlink and installer information to construct the install command with the --update parameter.


$ sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update


just logout and login in server to see updated version 2

aws-cli/2.11.13 Python/3.11.3 Linux/5.10.176-157.645.amzn2.x86_64 exe/x86_64.amzn.2 prompt/off


------------------------


---------------------------------

after this we need to install eksctl command

curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp

sudo mv /tmp/eksctl /usr/local/bin

eksctl version

---------------------------


now we install kubectl commands

curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.24.11/2023-03-17/bin/linux/amd64/kubectl

then we should move the folder

curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.24.10/2023-01-30/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/kubectl


-------------------------------------------------

now we should be auth to aws 

-- aws config

[ec2-user@ip-172-31-86-122 ~]$ aws configure
AWS Access Key ID [****************56XV]:
AWS Secret Access Key [****************a/V1]:
Default region name [us-east-1]:
Default output format [yaml]:


---------------------------------------------------

now we need to  create cluster --> every cluster as a one master node and nodes and master node connected to all the sepcific nodes

---how to create cluster
-- we  need a yaml file

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: spot-cluster
  region: us-east-1

managedNodeGroups:

# `instanceTypes` defaults to [`m5.large`] 
- name: cluster-1
  spot: true
  ssh:
    publicKeyName: Docker-Kubernates

----------------------

how to execte this --> vim eks-config.yaml

then create -- eksctl create cluster --config-file=[file-name].yaml

and to delete cluster -- eksctl delete cluster --config-file=[file-name].yaml


--------------------------------------------------------------------

now we can check in aws eks tool it as created. cluster created 2 spot instance
in server we can check once we trun eksctl create cluster

2023-04-18 05:59:13 [â„¹]  kubectl command should work with "/home/ec2-user/.kube/config", try 'kubectl get nodes'

---------------------------------------

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1EUXhPREExTlRBeE5sb1hEVE16TURReE5UQTFOVEF4Tmxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSmlZCmFOeVF3am9CK2EyRGhBVUxXalh2WlZhbVd1Nk9IS3RhMm96ejIySFNhM01GOTAzNWF2Qkhaemp6cHNjRkNud28KZ0Fwb3pldU9mOVpvTFNWditSUzVLN2ZRdjVOZmtweDVCVGtlZXJueHJITWRWVmdNK3UrTzY5YmdlWkRZLzRPNApMT2tSNU1vM05UZGgwbm4wU2NtWDFCSEd1ZHhjQUVuQUFtNXhJMGU4UFdvK1E1S29RQU0wMkIydjQ3b1hrWXlFCjNBMWV3MU1YRzFDUEZDaGs2VWZnWUxaZ3VBaElPL1dVc1ZoRWlvL3dMZWtIL2RTS01jSXhhVzFwaUoyTVZCYVcKOXp3S2l5OUpvTlFyaEJ2QmU1WWpRbXk4QXR3S0UyZ09OQ1Ura1NvTlpJbjEvalZrb3EySXliYWU4eVNaYzQyVQpjVUhuTE9SVDVCbHdWUEFtbFNFQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZPWjBTMXJVRXhTKzRBUzRpdWJRdjM3dUlzZlVNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBSTFEQU1iYVRCcUlZdlpOYmtPUwpaaHJsZlNQdE1FYmIxK2gzQkpuOFQ1OTY3bzYvcWl0OXFtMHUrdXRIMVlmREhtbUVqbUcwTlJsT0NxTTdtaVlDCnJsYWJNVE1BZUtkQzkxSkJDYU9NNVlLM1kwMnlyakdYWkNHR3B0VEk3bWVhKzcrZkhXazN4UDl4YWRNRDUvQmcKb0xOZ2h5THU5UkFYNHh4TXJMNXNTK1RudGtlL1B6dnRyeGQ4d1l4Y1poTWZDT3VCSHVjd2NJR2tubjR3cWJlMwpFOWVxYlVCS2pCRWdhbWFVeFB0VkZndlRYVCtoN2tqZGZhaGdFUHJ5Mzg2MjlQbWl0YnpEU3l3bzZtQ25LcjVHCk9nQThQbEMzSXEwN29qcnR2OVpEZzNWV0ZWNjhjM3F6Yy9iSHc2VFZTbkdqTjdEVGNhSEVlcVdrYUZJbG9qRkUKVnpVPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://0DD951546C3BD4B294B519F810090DEB.gr7.us-east-1.eks.amazonaws.com
  name: spot-cluster.us-east-1.eksctl.io
contexts:
- context:
    cluster: spot-cluster.us-east-1.eksctl.io
    user: Eks-Admin@spot-cluster.us-east-1.eksctl.io
  name: Eks-Admin@spot-cluster.us-east-1.eksctl.io
current-context: Eks-Admin@spot-cluster.us-east-1.eksctl.io
kind: Config
preferences: {}
users:
- name: Eks-Admin@spot-cluster.us-east-1.eksctl.io
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - eks
      - get-token
      - --cluster-name
"~/.kube/config" 31L, 2302B  

-------------------------------------------------------------------

-- kubectl get nodes   --> it give the all running nodes on server
-- kubectl get pods    --> it give the all pods info

--------------------------------------------

-- kubectl get namespace

    
NAME              STATUS   AGE
default           Active   6h16m
kube-node-lease   Active   6h16m
kube-public       Active   6h16m
kube-system       Active   6h16m


-----------------------------------
in Kubernates we gone do all thing with the yaml file..
lets create our first yaml file creation in server

in cmd type
--> vim namespace.yaml
--> paste the yaml file then save
--> for create the Kubernates resource
     -- kubectl create -f namespace.yaml
     ------
namespace/roboshop created

---------------------------------------------

now type --> kubectl get namespace
        
roboshop          Active   60s

--------------------------
To delete the resources
--> kubectl delete namespace.yaml 


-----------
we use more has instead of kubectl creat -f namespace.yaml--> we use kuubectl apply -f namespace.yaml 
beacuse it will give updated the resources, if resource is there it will not apply if not it will created