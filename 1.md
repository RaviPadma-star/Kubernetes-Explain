1. create a new iam user with admin access.. in real time we are not giving admin access save secret access key in local
2. create a workstation called a instance from there we should connect eks cluster means provision of cluster this is for only lab purpose
there is always a prodcution provision cluster
3. eksctl --> means its connect the aws console
4. cmd --> aws --version
 ---aws-cli/1.18.147 Python/2.7.18 Linux/5.10.176-157.645.amzn2.x86_64 botocore/1.18.6
here we getting aws v1 but need is aws 2 version for that search aws cli 2 version

-------------------

curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

Note
To update your current installation of the AWS CLI, add your existing symlink and installer information to construct the install command with the --update parameter.


$ sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update


just logout and login in server to see updated version 2

aws-cli/2.11.13 Python/3.11.3 Linux/5.10.176-157.645.amzn2.x86_64 exe/x86_64.amzn.2 prompt/off


------------------------


---------------------------------

after this we need to install eksctl command

curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp

sudo mv /tmp/eksctl /usr/local/bin

eksctl version

---------------------------


now we install kubectl commands

curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.24.11/2023-03-17/bin/linux/amd64/kubectl

then we should move the folder

curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.24.10/2023-01-30/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/kubectl


-------------------------------------------------

now we should be auth to aws 

-- aws config

[ec2-user@ip-172-31-86-122 ~]$ aws configure
AWS Access Key ID [****************56XV]:
AWS Secret Access Key [****************a/V1]:
Default region name [us-east-1]:
Default output format [yaml]:


---------------------------------------------------

now we need to  create cluster --> every cluster as a one master node and nodes and master node connected to all the sepcific nodes

---how to create cluster
-- we  need a yaml file

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: spot-cluster
  region: us-east-1

managedNodeGroups:

# `instanceTypes` defaults to [`m5.large`] 
- name: cluster-1
  spot: true
  ssh:
    publicKeyName: Docker-Kubernates

----------------------

how to execte this --> vim eks-config.yaml

then create -- eksctl create cluster --config-file=[file-name].yaml

and to delete cluster -- eksctl delete cluster --config-file=[file-name].yaml


--------------------------------------------------------------------

now we can check in aws eks tool it as created. cluster created 2 spot instance
in server we can check once we trun eksctl create cluster

2023-04-18 05:59:13 [â„¹]  kubectl command should work with "/home/ec2-user/.kube/config", try 'kubectl get nodes'

---------------------------------------

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1EUXhPREExTlRBeE5sb1hEVE16TURReE5UQTFOVEF4Tmxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSmlZCmFOeVF3am9CK2EyRGhBVUxXalh2WlZhbVd1Nk9IS3RhMm96ejIySFNhM01GOTAzNWF2Qkhaemp6cHNjRkNud28KZ0Fwb3pldU9mOVpvTFNWditSUzVLN2ZRdjVOZmtweDVCVGtlZXJueHJITWRWVmdNK3UrTzY5YmdlWkRZLzRPNApMT2tSNU1vM05UZGgwbm4wU2NtWDFCSEd1ZHhjQUVuQUFtNXhJMGU4UFdvK1E1S29RQU0wMkIydjQ3b1hrWXlFCjNBMWV3MU1YRzFDUEZDaGs2VWZnWUxaZ3VBaElPL1dVc1ZoRWlvL3dMZWtIL2RTS01jSXhhVzFwaUoyTVZCYVcKOXp3S2l5OUpvTlFyaEJ2QmU1WWpRbXk4QXR3S0UyZ09OQ1Ura1NvTlpJbjEvalZrb3EySXliYWU4eVNaYzQyVQpjVUhuTE9SVDVCbHdWUEFtbFNFQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZPWjBTMXJVRXhTKzRBUzRpdWJRdjM3dUlzZlVNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBSTFEQU1iYVRCcUlZdlpOYmtPUwpaaHJsZlNQdE1FYmIxK2gzQkpuOFQ1OTY3bzYvcWl0OXFtMHUrdXRIMVlmREhtbUVqbUcwTlJsT0NxTTdtaVlDCnJsYWJNVE1BZUtkQzkxSkJDYU9NNVlLM1kwMnlyakdYWkNHR3B0VEk3bWVhKzcrZkhXazN4UDl4YWRNRDUvQmcKb0xOZ2h5THU5UkFYNHh4TXJMNXNTK1RudGtlL1B6dnRyeGQ4d1l4Y1poTWZDT3VCSHVjd2NJR2tubjR3cWJlMwpFOWVxYlVCS2pCRWdhbWFVeFB0VkZndlRYVCtoN2tqZGZhaGdFUHJ5Mzg2MjlQbWl0YnpEU3l3bzZtQ25LcjVHCk9nQThQbEMzSXEwN29qcnR2OVpEZzNWV0ZWNjhjM3F6Yy9iSHc2VFZTbkdqTjdEVGNhSEVlcVdrYUZJbG9qRkUKVnpVPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://0DD951546C3BD4B294B519F810090DEB.gr7.us-east-1.eks.amazonaws.com
  name: spot-cluster.us-east-1.eksctl.io
contexts:
- context:
    cluster: spot-cluster.us-east-1.eksctl.io
    user: Eks-Admin@spot-cluster.us-east-1.eksctl.io
  name: Eks-Admin@spot-cluster.us-east-1.eksctl.io
current-context: Eks-Admin@spot-cluster.us-east-1.eksctl.io
kind: Config
preferences: {}
users:
- name: Eks-Admin@spot-cluster.us-east-1.eksctl.io
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - eks
      - get-token
      - --cluster-name
"~/.kube/config" 31L, 2302B  

-------------------------------------------------------------------

-- kubectl get nodes   --> it give the all running nodes on server
-- kubectl get pods    --> it give the all pods info

--------------------------------------------

-- kubectl get namespace

    
NAME              STATUS   AGE
default           Active   6h16m
kube-node-lease   Active   6h16m
kube-public       Active   6h16m
kube-system       Active   6h16m


-----------------------------------
in Kubernates we gone do all thing with the yaml file..
lets create our first yaml file creation in server

in cmd type
--> vim namespace.yaml
--> paste the yaml file then save
--> for create the Kubernates resource
     -- kubectl create -f namespace.yaml
     ------
namespace/roboshop created

---------------------------------------------

now type --> kubectl get namespace
        
roboshop          Active   60s

--------------------------
To delete the resources
--> kubectl delete -f namespace.yaml 


-----------
we use more has instead of kubectl creat -f namespace.yaml--> we use kuubectl apply -f namespace.yaml 
beacuse it will give updated the resources, if resource is there it will not apply if not it will created


----------------
now we created pod.yaml file to create a pod
-- vim Pod.yaml
-- paste the pod.yaml
-- run -> kubectl apply -f Pod.yaml
--   pod/nginx created


then --> kubectl get pods
[ec2-user@ip-172-31-86-122 ~]$ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          2m20s

--> pods are running but we need to lnow it exactly running for that we have
--> kubectl get pods -o wide 

  [ec2-user@ip-172-31-86-122 ~]$ kubectl get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE     IP              NODE                             NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          3m53s   192.168.50.63   ip-192-168-50-243.ec2.internal   <none>           <none>
[ec2-user@ip-172-31-86-122 ~]$


--> also we can check yaml file 
        -- kubectl get pods -o yaml

--> we created a namespace as robshop its like network creation docker to attach this in Pods.yaml file edit and under the metadata --> give 
namespace: roboshop 
then --> kubectl apply -f Pod.yaml
-- it says pods/nginx created

-- then grt info --> kubectl get ns roboshop
NAME       STATUS   AGE
roboshop   Active   18m


-------------------
to get all running pods with namespace
--> kubectl get pods -A



[ec2-user@ip-172-31-86-122 ~]$ kubectl get pods -A
NAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE
default       nginx                      1/1     Running   0          11m
kube-system   aws-node-fggrf             1/1     Running   0          24m
kube-system   aws-node-wq4d7             1/1     Running   0          2m27s
kube-system   coredns-7975d6fb9b-svd2m   1/1     Running   0          8m14s
kube-system   coredns-7975d6fb9b-xjl8q   1/1     Running   0          8m14s
kube-system   kube-proxy-frvj7           1/1     Running   0          24m
kube-system   kube-proxy-q8bv8           1/1     Running   0          2m27s
roboshop      nginx                      1/1     Running   0          3m31s


----------------------------------
to delete pods.yaml and pods
-- kubectl delete -f Pod.yaml
-- kubectl delete pod nginx

-----------------------------
let us create --> kubectl apply -f pod.yaml
after creating it check on instance copy public ip and run putty wheater we have access or not

in our work station server type: to get attaced namespace
-- kubectl get pods -n roboshop -o wide 

nginx   1/1     Running   0          4m40s   192.168.46.201   ip-192-168-47-52.ec2.internal   <none>           <none>

then--> copy the ip address and go to spot-instace server then paste as --> curl 192.168.46.201



[ec2-user@ip-192-168-50-243 ~]$ curl 192.168.46.201
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>


--------------------
for now can not access this outside of nodes